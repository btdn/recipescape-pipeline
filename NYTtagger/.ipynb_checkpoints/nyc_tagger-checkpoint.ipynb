{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a notebook showing a modification of the original [NYT Ingredient Phrase tagger](https://github.com/NYTimes/ingredient-phrase-tagger). [Here](http://open.blogs.nytimes.com/2016/04/27/structured-ingredients-data-tagging/) is the article where they talk about it.\n",
    "\n",
    "That github repository contains New York Time's tool for performing Named Entity Recognition via Conditional Random Fields on food recipes to extract the ingredients used on those recipes as well as the quantities.\n",
    "\n",
    "On their implementation they use a [CRF++](https://taku910.github.io/crfpp/) as the extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will use pycrfsuite instead of CRF++, the main reasons being:\n",
    "\n",
    "* by using a full python solution (even though pycrfsuite is just a wrapper around [crfsuite](http://www.chokkan.org/software/crfsuite/)) we can deploy the model more easily, and \n",
    "\n",
    "* installing CRF++ proved to be a challenge in Ubuntu 14.04\n",
    "\n",
    "You can install pycrfsuite by doing:\n",
    "\n",
    "`pip install python-crfsuite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the train_file with features produced by calling *(as it appears on the README)*:\n",
    "\n",
    "```\n",
    "bin/generate_data --data-path=input.csv --count=180000 --offset=0 > tmp/train_file\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "from itertools import chain\n",
    "import nltk\n",
    "import pycrfsuite\n",
    "\n",
    "from lib.training import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp/train_file') as fname:\n",
    "    lines = fname.readlines()\n",
    "    items = [line.strip('\\n').split('\\t') for line in lines]\n",
    "    items = [item for item in items if len(item)==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1$1/4', 'I1', 'L20', 'NoCAP', 'NoPAREN', 'B-QTY'],\n",
       " ['cups', 'I2', 'L20', 'NoCAP', 'NoPAREN', 'B-UNIT'],\n",
       " ['cooked', 'I3', 'L20', 'NoCAP', 'NoPAREN', 'B-COMMENT'],\n",
       " ['and', 'I4', 'L20', 'NoCAP', 'NoPAREN', 'I-COMMENT'],\n",
       " ['pureed', 'I5', 'L20', 'NoCAP', 'NoPAREN', 'I-COMMENT'],\n",
       " ['fresh', 'I6', 'L20', 'NoCAP', 'NoPAREN', 'I-COMMENT'],\n",
       " ['butternut', 'I7', 'L20', 'NoCAP', 'NoPAREN', 'B-NAME'],\n",
       " ['squash', 'I8', 'L20', 'NoCAP', 'NoPAREN', 'I-NAME'],\n",
       " [',', 'I9', 'L20', 'NoCAP', 'NoPAREN', 'OTHER'],\n",
       " ['or', 'I10', 'L20', 'NoCAP', 'NoPAREN', 'I-COMMENT']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each line of the train_file follows the format:\n",
    "\n",
    "- token\n",
    "- position on the phrase. (I1 would be first word, I2 the second, and so on)\n",
    "- LX , being the length group of the token (defined by [LengthGroup](https://github.com/NYTimes/ingredient-phrase-tagger/blob/master/lib/training/utils.py#L140))\n",
    "- NoCAP or YesCAP, whether the token is capitalized or not\n",
    "- YesParen or NoParen, whether the token is inside parenthesis or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCRFSuite expects the input to be a list of the structured items and their respective tags. So we process the items from the train file and bucket them into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19948"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "sent = [items[0]]\n",
    "for item in items[1:]:\n",
    "    if 'I1' in item:\n",
    "        sentences.append(sent)\n",
    "        sent = [item]\n",
    "    else:\n",
    "        sent.append(item)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(sentences)\n",
    "test_size = 0.1\n",
    "data_size = len(sentences)\n",
    "\n",
    "test_data = sentences[:int(test_size*data_size)]\n",
    "train_data = sentences[int(test_size*data_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Freshly', 'I1', 'L8', 'YesCAP', 'NoPAREN'],\n",
       " ['ground', 'I2', 'L8', 'NoCAP', 'NoPAREN'],\n",
       " ['pepper', 'I3', 'L8', 'NoCAP', 'NoPAREN'],\n",
       " ['to', 'I4', 'L8', 'NoCAP', 'NoPAREN'],\n",
       " ['taste', 'I5', 'L8', 'NoCAP', 'NoPAREN']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sent2labels(sent):\n",
    "    return [word[-1] for word in sent]\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word[:-1] for word in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word[0] for word in sent]   \n",
    "\n",
    "y_train = [sent2labels(s) for s in train_data]\n",
    "X_train = [sent2features(s) for s in train_data]\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We set up the CRF trainer. We will use the default values and include all the possible joint features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained the following hyperparameters by performing a GridSearchCV with the scikit learn implementation of pycrfsuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.set_params(\n",
    "{\n",
    "        'c1': 0.43,\n",
    "        'c2': 0.012,\n",
    "        'max_iterations': 100,\n",
    "        'feature.possible_transitions': True,\n",
    "        'feature.possible_states': True,\n",
    "        'linesearch': 'StrongBacktracking'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model (this might take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.train('tmp/trained_pycrfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a pretrained model that we can just deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x115b07e50>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('tmp/trained_pycrfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just add a wrapper function for the script found in **lib/testing/convert_to_json.py** and create a convient way to parse an ingredient sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from lib.training import utils\n",
    "from string import punctuation\n",
    "import spacy.en\n",
    "\n",
    "nlp_engine = spacy.en.English()\n",
    "\n",
    "#from nltk.tokenize import PunktSentenceTokenizer\n",
    "#tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "def get_sentence_features(sent):\n",
    "    \"\"\"Gets  the features of the sentence\"\"\"\n",
    "    sent_tokens = utils.tokenize(utils.cleanUnicodeFractions(sent))\n",
    "\n",
    "    sent_features = []\n",
    "    for i, token in enumerate(sent_tokens):\n",
    "        token_features = [token]\n",
    "        token_features.extend(utils.getFeatures(token, i+1, sent_tokens))\n",
    "        sent_features.append(token_features)\n",
    "    return sent_features\n",
    "\n",
    "def format_ingredient_output(tagger_output, display=False):\n",
    "    \"\"\"Formats the tagger output into a more convenient dictionary\"\"\"\n",
    "    data = [{}]\n",
    "    display = [[]]\n",
    "    prevTag = None\n",
    "\n",
    "\n",
    "    for token, tag in tagger_output:\n",
    "    # turn B-NAME/123 back into \"name\"\n",
    "        tag = re.sub(r'^[BI]\\-', \"\", tag).lower()\n",
    "\n",
    "        # ---- DISPLAY ----\n",
    "        # build a structure which groups each token by its tag, so we can\n",
    "        # rebuild the original display name later.\n",
    "\n",
    "        if prevTag != tag:\n",
    "            display[-1].append((tag, [token]))\n",
    "            prevTag = tag\n",
    "        else:\n",
    "            display[-1][-1][1].append(token)\n",
    "            #               ^- token\n",
    "            #            ^---- tag\n",
    "            #        ^-------- ingredient\n",
    "\n",
    "            # ---- DATA ----\n",
    "            # build a dict grouping tokens by their tag\n",
    "\n",
    "            # initialize this attribute if this is the first token of its kind\n",
    "        if tag not in data[-1]:\n",
    "            data[-1][tag] = []\n",
    "\n",
    "        # HACK: If this token is a unit, singularize it so Scoop accepts it.\n",
    "        if tag == \"unit\":\n",
    "            token = utils.singularize(token)\n",
    "\n",
    "        data[-1][tag].append(token)\n",
    "\n",
    "    # reassemble the output into a list of dicts.\n",
    "    output = [\n",
    "        dict([(k, utils.smartJoin(tokens)) for k, tokens in ingredient.iteritems()])\n",
    "        for ingredient in data\n",
    "        if len(ingredient)\n",
    "    ]\n",
    "\n",
    "    # Add the raw ingredient phrase\n",
    "    for i, v in enumerate(output):\n",
    "        output[i][\"input\"] = utils.smartJoin(\n",
    "            [\" \".join(tokens) for k, tokens in display[i]])\n",
    "\n",
    "    return output\n",
    "\n",
    "def parse_ingredient(sent):\n",
    "    \"\"\"ingredient parsing logic\"\"\"\n",
    "    sentence_features = get_sentence_features(sent)\n",
    "    tags = tagger.tag(sentence_features)\n",
    "    tagger_output = zip(sent2tokens(sentence_features), tags)\n",
    "    parsed_ingredient =  format_ingredient_output(tagger_output)\n",
    "    if parsed_ingredient:\n",
    "        parsed_ingredient[0]['name'] = parsed_ingredient[0].get('name','').strip('.')\n",
    "    return parsed_ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('../chocolate_chip_cookie/ButterySugarCookies.json') as data_file: \n",
    "    # keep abs path for now\n",
    "    # must go thru all files in the directory ***\n",
    "    data = json.load(data_file)\n",
    "\n",
    "ingre_text = data[\"ingredients\"]\n",
    "steps_text = data[\"instructions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_recipe_ingredients(ingredient_list):\n",
    "    \"\"\"Wrapper around parse_ingredient so we can call it on an ingredient list\"\"\"\n",
    "    #sentences = tokenizer.tokenize(q)\n",
    "    sentences = tokenizer.tokenize(ingredient_list)\n",
    "    sentences = [sent.strip('\\n') for sent in sentences]\n",
    "    ingredients = []\n",
    "    for sent in sentences:\n",
    "        ingredients.extend(parse_ingredient(sent))\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingred_dict=[]\n",
    "for ingr in ingre_text:\n",
    "    #ingr = ingre_text[2]\n",
    "    #print ingr\n",
    "    ingr_parse = parse_recipe_ingredients(ingr)\n",
    "    #print ingr_parse\n",
    "    for element in ingr_parse:\n",
    "        ingred_dict.append(ingr_parse[0]['name'])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'unsalted butter',\n",
       " u'sugar',\n",
       " u'salt',\n",
       " u'egg',\n",
       " u'vanilla extract',\n",
       " u'all-purpose flour',\n",
       " u'sugar',\n",
       " u'**Special equipment:** Wax paper; 2']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beat 97 VERB\n",
      "together 84 ADV\n",
      "butter 89 NOUN\n",
      ", 94 PUNCT\n",
      "sugar 89 NOUN\n",
      ", 94 PUNCT\n",
      "and 86 CONJ\n",
      "salt 89 NOUN\n",
      "in 83 ADP\n",
      "a 87 DET\n",
      "large 82 ADJ\n",
      "bowl 89 NOUN\n",
      "with 83 ADP\n",
      "an 87 DET\n",
      "electric 82 ADJ\n",
      "mixer 89 NOUN\n",
      "at 83 ADP\n",
      "medium 82 ADJ\n",
      "- 94 PUNCT\n",
      "high 82 ADJ\n",
      "speed 89 NOUN\n",
      "until 83 ADP\n",
      "pale 82 ADJ\n",
      "and 86 CONJ\n",
      "fluffy 82 ADJ\n",
      ", 94 PUNCT\n",
      "about 84 ADV\n",
      "3 90 NUM\n",
      "minutes 89 NOUN\n",
      "in 83 ADP\n",
      "a 87 DET\n",
      "stand 89 NOUN\n",
      "mixer 89 NOUN\n",
      "( 94 PUNCT\n",
      "preferably 84 ADV\n",
      "fitted 97 VERB\n",
      "with 83 ADP\n",
      "paddle 89 NOUN\n",
      "attachment 89 NOUN\n",
      ") 94 PUNCT\n",
      "or 86 CONJ\n",
      "6 90 NUM\n",
      "with 83 ADP\n",
      "a 87 DET\n",
      "handheld 89 NOUN\n",
      ". 94 PUNCT\n",
      "Beat 89 NOUN\n",
      "in 83 ADP\n",
      "egg 89 NOUN\n",
      "and 86 CONJ\n",
      "vanilla 89 NOUN\n",
      ". 94 PUNCT\n",
      "Reduce 82 ADJ\n",
      "speed 89 NOUN\n",
      "to 91 PART\n",
      "low 84 ADV\n",
      ", 94 PUNCT\n",
      "then 84 ADV\n",
      "mix 97 VERB\n",
      "in 83 ADP\n",
      "flour 89 NOUN\n",
      ". 94 PUNCT\n",
      "Halve 97 VERB\n",
      "dough 89 NOUN\n",
      "and 86 CONJ\n",
      "form 97 VERB\n",
      "each 87 DET\n",
      "half 89 NOUN\n",
      "into 83 ADP\n",
      "a 87 DET\n",
      "disk 89 NOUN\n",
      ", 94 PUNCT\n",
      "then 84 ADV\n",
      "wrap 97 VERB\n",
      "in 83 ADP\n",
      "wax 89 NOUN\n",
      "paper 89 NOUN\n",
      ". 94 PUNCT\n",
      "Put 97 VERB\n",
      "each 87 DET\n",
      "disk 89 NOUN\n",
      "in 83 ADP\n",
      "a 87 DET\n",
      "resealable 82 ADJ\n",
      "plastic 82 ADJ\n",
      "bag 89 NOUN\n",
      "and 86 CONJ\n",
      "chill 89 NOUN\n",
      "until 83 ADP\n",
      "firm 82 ADJ\n",
      "enough 84 ADV\n",
      "to 91 PART\n",
      "roll 97 VERB\n",
      "into 83 ADP\n",
      "balls 89 NOUN\n",
      ", 94 PUNCT\n",
      "about 84 ADV\n",
      "1 90 NUM\n",
      "hour 89 NOUN\n",
      ". 94 PUNCT\n",
      "Heat 97 VERB\n",
      "oven 89 NOUN\n",
      "to 83 ADP\n",
      "350°F 97 VERB\n",
      "with 83 ADP\n",
      "rack 89 NOUN\n",
      "in 83 ADP\n",
      "middle 89 NOUN\n",
      ". 94 PUNCT\n",
      "Line 89 NOUN\n",
      "baking 89 NOUN\n",
      "sheets 89 NOUN\n",
      "with 83 ADP\n",
      "parchment 89 NOUN\n",
      "paper 89 NOUN\n",
      ". 94 PUNCT\n",
      "While 83 ADP\n",
      "oven 82 ADJ\n",
      "heats 89 NOUN\n",
      ", 94 PUNCT\n",
      "work 89 NOUN\n",
      "with 83 ADP\n",
      "1 90 NUM\n",
      "piece 89 NOUN\n",
      "of 83 ADP\n",
      "dough 89 NOUN\n",
      "( 94 PUNCT\n",
      "keep 97 VERB\n",
      "remaining 97 VERB\n",
      "dough 89 NOUN\n",
      "chilled 97 VERB\n",
      ") 94 PUNCT\n",
      ". 94 PUNCT\n",
      "Roll 93 PROPN\n",
      "1 90 NUM\n",
      "level 89 NOUN\n",
      "tablespoon 89 NOUN\n",
      "of 83 ADP\n",
      "dough 89 NOUN\n",
      "into 83 ADP\n",
      "a 87 DET\n",
      "ball 89 NOUN\n",
      ", 94 PUNCT\n",
      "then 84 ADV\n",
      "roll 97 VERB\n",
      "in 84 ADV\n",
      "coarse 82 ADJ\n",
      "sugar 89 NOUN\n",
      "in 83 ADP\n",
      "a 87 DET\n",
      "shallow 82 ADJ\n",
      "bowl 89 NOUN\n",
      "to 83 ADP\n",
      "coat 89 NOUN\n",
      "completely 84 ADV\n",
      ". 94 PUNCT\n",
      "( 94 PUNCT\n",
      "If 83 ADP\n",
      "dough 89 NOUN\n",
      "becomes 97 VERB\n",
      "too 84 ADV\n",
      "soft 82 ADJ\n",
      "to 91 PART\n",
      "roll 97 VERB\n",
      "easily 84 ADV\n",
      "into 83 ADP\n",
      "balls 89 NOUN\n",
      ", 94 PUNCT\n",
      "quick 82 ADJ\n",
      "- 94 PUNCT\n",
      "chill 89 NOUN\n",
      "in 83 ADP\n",
      "the 87 DET\n",
      "freezer 89 NOUN\n",
      "or 86 CONJ\n",
      "chill 89 NOUN\n",
      "in 83 ADP\n",
      "the 87 DET\n",
      "refrigerator 89 NOUN\n",
      ". 94 PUNCT\n",
      ") 94 PUNCT\n",
      "Place 89 NOUN\n",
      "balls 89 NOUN\n",
      "2 90 NUM\n",
      "inches 89 NOUN\n",
      "apart 84 ADV\n",
      "on 83 ADP\n",
      "a 87 DET\n",
      "lined 82 ADJ\n",
      "baking 89 NOUN\n",
      "sheet 89 NOUN\n",
      ". 94 PUNCT\n",
      "With 83 ADP\n",
      "the 87 DET\n",
      "flat 82 ADJ\n",
      "bottom 89 NOUN\n",
      "of 83 ADP\n",
      "a 87 DET\n",
      "glass 89 NOUN\n",
      ", 94 PUNCT\n",
      "flatten 97 VERB\n",
      "balls 89 NOUN\n",
      "into 83 ADP\n",
      "2-inch 90 NUM\n",
      "rounds 89 NOUN\n",
      ". 94 PUNCT\n",
      "Bake 93 PROPN\n",
      "cookies 89 NOUN\n",
      ", 94 PUNCT\n",
      "one 90 NUM\n",
      "sheet 89 NOUN\n",
      "at 83 ADP\n",
      "time 89 NOUN\n",
      ", 94 PUNCT\n",
      "until 83 ADP\n",
      "bottoms 89 NOUN\n",
      "are 97 VERB\n",
      "golden 82 ADJ\n",
      ", 94 PUNCT\n",
      "12 90 NUM\n",
      "to 91 PART\n",
      "15 90 NUM\n",
      "minutes 89 NOUN\n",
      "total 82 ADJ\n",
      ". 94 PUNCT\n",
      "Cool 89 NOUN\n",
      "on 83 ADP\n",
      "sheets 89 NOUN\n",
      "2 90 NUM\n",
      "minutes 89 NOUN\n",
      ", 94 PUNCT\n",
      "then 84 ADV\n",
      "transfer 97 VERB\n",
      "with 83 ADP\n",
      "a 87 DET\n",
      "metal 89 NOUN\n",
      "spatula 89 NOUN\n",
      "to 83 ADP\n",
      "racks 89 NOUN\n",
      "to 91 PART\n",
      "cool 97 VERB\n",
      "completely 84 ADV\n",
      ". 94 PUNCT\n",
      "Make 97 VERB\n",
      "more 82 ADJ\n",
      "cookies 89 NOUN\n",
      "with 83 ADP\n",
      "remaining 97 VERB\n",
      "dough 89 NOUN\n",
      "on 83 ADP\n",
      "cooled 97 VERB\n",
      "baking 97 VERB\n",
      "sheets 89 NOUN\n",
      ". 94 PUNCT\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(steps_text)):\n",
    "    sent_tokens = nlp_engine(steps_text[i])\n",
    "    for sent in sent_tokens.sents:\n",
    "        for token in sent:\n",
    "            print token, token.pos, token.pos_\n",
    "        print \"====================\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
