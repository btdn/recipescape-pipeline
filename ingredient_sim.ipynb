{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pprint as pp\n",
    "\n",
    "from anytree import RenderTree\n",
    "from zss import *\n",
    "\n",
    "from retrieve_recipes import *\n",
    "from im2recipe_w2v_weight import *\n",
    "\n",
    "recipes = get_recipes_json('chocochip') #list\n",
    "\n",
    "def print_tree(tree):\n",
    "    for pre, fill, node in RenderTree(tree):\n",
    "        print(\"%s%s\" % (pre, node.label))\n",
    "\n",
    "\n",
    "#pairwise Jaccard Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from NYTtagger.lib.training import utils\n",
    "from string import punctuation\n",
    "\n",
    "import pycrfsuite\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('NYTtagger/tmp/trained_pycrfsuite')\n",
    "\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [word[-1] for word in sent]\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word[:-1] for word in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word[0] for word in sent]   \n",
    "\n",
    "def get_sentence_features(sent):\n",
    "    \"\"\"Gets  the features of the sentence\"\"\"\n",
    "    sent_tokens = list(utils.tokenize(utils.cleanUnicodeFractions(sent)))\n",
    "\n",
    "    sent_features = []\n",
    "    for i, token in enumerate(sent_tokens):\n",
    "        token_features = [token]\n",
    "        token_features.extend(utils.getFeatures(token, i+1, list(sent_tokens)))\n",
    "        sent_features.append(token_features)\n",
    "    return sent_features\n",
    "\n",
    "def format_ingredient_output(tagger_output, display=False):\n",
    "    \"\"\"Formats the tagger output into a more convenient dictionary\"\"\"\n",
    "    data = [{}]\n",
    "    display = [[]]\n",
    "    prevTag = None\n",
    "\n",
    "    for token, tag in tagger_output:\n",
    "    # turn B-NAME/123 back into \"name\"\n",
    "#        tag = re.sub(r'^[BI]\\-', \"\", tag).lower()\n",
    "\n",
    "        # ---- DISPLAY ----\n",
    "        # build a structure which groups each token by its tag, so we can\n",
    "        # rebuild the original display name later.\n",
    "\n",
    "        if prevTag != tag:\n",
    "            display[-1].append((tag, [token]))\n",
    "            prevTag = tag\n",
    "        else:\n",
    "            display[-1][-1][1].append(token)\n",
    "            #               ^- token\n",
    "            #            ^---- tag\n",
    "            #        ^-------- ingredient\n",
    "\n",
    "            # ---- DATA ----\n",
    "            # build a dict grouping tokens by their tag\n",
    "\n",
    "            # initialize this attribute if this is the first token of its kind\n",
    "        if tag not in data[-1]:\n",
    "            data[-1][tag] = []\n",
    "\n",
    "        # HACK: If this token is a unit, singularize it so Scoop accepts it.\n",
    "        if tag == \"unit\":\n",
    "            token = utils.singularize(token)\n",
    "\n",
    "        data[-1][tag].append(token)\n",
    "\n",
    "    # reassemble the output into a list of dicts.\n",
    "    output = [\n",
    "        dict([(k, utils.smartJoin(tokens)) for k, tokens in ingredient.items()])\n",
    "        for ingredient in data\n",
    "        if len(ingredient)\n",
    "    ]\n",
    "\n",
    "    # Add the raw ingredient phrase\n",
    "    for i, v in enumerate(output):\n",
    "        output[i][\"input\"] = utils.smartJoin(\n",
    "            [\" \".join(tokens) for k, tokens in display[i]])\n",
    "\n",
    "    return output\n",
    "\n",
    "def parse_ingredient(sent):\n",
    "    \"\"\"ingredient parsing logic\"\"\"\n",
    "    sentence_features = get_sentence_features(sent)\n",
    "    tags = tagger.tag(sentence_features)\n",
    "    tagger_output = zip(sent2tokens(sentence_features), tags)\n",
    "    parsed_ingredient =  format_ingredient_output(tagger_output)\n",
    "    if parsed_ingredient:\n",
    "        parsed_ingredient[0]['name'] = parsed_ingredient[0].get('name','').strip('.')\n",
    "    return parsed_ingredient\n",
    "\n",
    "def parse_recipe_ingredients(ingredient_list):\n",
    "    \"\"\"Wrapper around parse_ingredient so we can call it on an ingredient list\"\"\"\n",
    "    sentences = tokenizer.tokenize(ingredient_list)\n",
    "    sentences = [sent.strip('\\n') for sent in sentences]\n",
    "    ingredients = []\n",
    "    for sent in sentences:\n",
    "        ingredients.extend(parse_ingredient(sent))\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n"
     ]
    }
   ],
   "source": [
    "ids=[]\n",
    "for recipe in recipes:\n",
    "    ids.append(recipe['origin_id'])\n",
    "\n",
    "ids = list(set(ids)) #how many recipes - determined by distict IDs\n",
    "print(len(ids))\n",
    "\n",
    "rec_ids = np.matrix(ids)\n",
    "rec_ids.shape\n",
    "rec_ids.dump(\"recipe_ids.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minsuk/.pyenv/versions/3.6.2/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "ingre_lists = []\n",
    "for recipe_id in ids:\n",
    "    for recipe in recipes:\n",
    "        if recipe['origin_id'] == recipe_id:\n",
    "            ingre_str = \". \".join(recipe['ingredients'])\n",
    "            parsed_ingre_list = parse_recipe_ingredients(ingre_str)\n",
    "\n",
    "            ingre_store = []\n",
    "            for ingre in parsed_ingre_list:\n",
    "                if not \"I-NAME\" in ingre:\n",
    "                    ingre[\"I-NAME\"] = \"\"\n",
    "                if not \"B-NAME\" in ingre:\n",
    "                    ingre[\"B-NAME\"] = \"\"\n",
    "                ingre_text = ingre['B-NAME'] + \" \"+ ingre[\"I-NAME\"]\n",
    "                ingre_store.append(ingre_text)\n",
    "\n",
    "            #print(ingre_store)\n",
    "            item={\"id\":recipe['origin_id'], \"ingredients\": ingre_store}\n",
    "            ingre_lists.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n",
      "CPU times: user 1.06 ms, sys: 2.93 ms, total: 3.99 ms\n",
      "Wall time: 2.83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dim = len(ingre_lists)\n",
    "print(dim)\n",
    "ingre_dist_matrix = np.zeros((dim, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    "  \n",
    "def jaccard_similarity(x,y):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13333333333333333"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(ingre_lists[0]['ingredients'], ingre_lists[9]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 333 ms, sys: 3.61 ms, total: 336 ms\n",
      "Wall time: 336 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(dim):\n",
    "    for j in range(i+1,dim):\n",
    "        jac_sim = jaccard_similarity(ingre_lists[i]['ingredients'], ingre_lists[j]['ingredients'])\n",
    "        ingre_dist_matrix[i][j]=jac_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = np.matrix(ingre_dist_matrix)\n",
    "mat.dump(\"ingre_dist_matrix.dat\")\n",
    "#mat2 = numpy.load(\"my_matrix.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
